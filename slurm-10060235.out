/cluster/home/oilter/begin.sh: line 1: env2lmod: command not found
Namespace(cfg='/cluster/home/oilter/PIDNet/configs/p4b/pidnet_small_p4b_storage.yaml', opts=[], seed=304)
AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: p4b
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 19
  ROOT: /cluster/scratch/oilter/data/
  TEST_SET: list/p4b/train.lst
  TRAIN_SET: list/p4b/train.lst
GPUS: (0, 1)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.9
  SB_WEIGHTS: 1.0
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: /cluster/home/oilter/PIDNet/pretrained_models/cityscapes/PIDNet_S_Cityscapes_test.pt
OUTPUT_DIR: /cluster/scratch/oilter/output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1920
  BATCH_SIZE_PER_GPU: 1
  FLIP_TEST: False
  IMAGE_SIZE: [1920, 1080]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 1920
  BATCH_SIZE_PER_GPU: 4
  BEGIN_EPOCH: 0
  END_EPOCH: 484
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 255
  IMAGE_SIZE: [1920, 1080]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
Attention!!!
Loaded 479 parameters!
Over!!!
Seeding with 304
=> creating /cluster/scratch/oilter/output/p4b/pidnet_small_p4b_storage
=> creating log/p4b/pidnet_small/pidnet_small_p4b_storage_2023-02-18-15-49
/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
Traceback (most recent call last):
  File "/cluster/home/oilter/PIDNet/tools/train.py", line 218, in <module>
    main()
  File "/cluster/home/oilter/PIDNet/tools/train.py", line 182, in main
    trainloader, optimizer, model, writer_dict)
  File "/cluster/home/oilter/PIDNet/tools/../utils/function.py", line 43, in train
    losses, _, acc, loss_list = model(images, labels, bd_gts)
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 169, in forward
    return self.gather(outputs, self.output_device)
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 181, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 78, in gather
    res = gather_map(outputs)
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 73, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 73, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 63, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/cluster/home/oilter/miniconda3/envs/ss/lib/python3.7/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
RuntimeError: CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 10.76 GiB total capacity; 7.84 GiB already allocated; 1.03 GiB free; 8.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
